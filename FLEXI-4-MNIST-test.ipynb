{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = 'MNIST'\n",
    "data_path = 'raw/'\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 499 weights that need to be deployed.\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# Initialize the model with the given configuration and number of classes, and move it to the specified device\n",
    "model = NET_4k_MNIST(num_classes).to(device)\n",
    "\n",
    "total_weights = count_weights(model)\n",
    "print(f\"The network has {total_weights} weights that need to be deployed.\")\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "model = torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET_4k_MNIST(\n",
      "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (conv1_depthwise): QuantizedConv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), scale=0.49285900592803955, zero_point=88, padding=(1, 1), dilation=(2, 2))\n",
      "  (conv1_pointwise): QuantizedConv2d(1, 8, kernel_size=(1, 1), stride=(1, 1), scale=4.715211868286133, zero_point=61)\n",
      "  (bn1): QuantizedBatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): QuantizedHardswish()\n",
      "  (conv2_depthwise): QuantizedConv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), scale=1.147164225578308, zero_point=90, padding=(1, 1), dilation=(2, 2), groups=8)\n",
      "  (conv2_pointwise): QuantizedConv2d(8, 10, kernel_size=(1, 1), stride=(1, 1), scale=8.301703453063965, zero_point=80)\n",
      "  (bn2): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): QuantizedHardswish()\n",
      "  (conv3_depthwise): QuantizedConv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), scale=1.6483409404754639, zero_point=58, padding=(1, 1), groups=10)\n",
      "  (conv3_pointwise): QuantizedConv2d(10, 12, kernel_size=(1, 1), stride=(1, 1), scale=13.042365074157715, zero_point=61)\n",
      "  (bn3): QuantizedBatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation3): QuantizedHardswish()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): QuantizedLinear(in_features=12, out_features=10, scale=0.41189584136009216, zero_point=92, qscheme=torch.per_channel_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "# Specify the filename of the saved model\n",
    "model_filename = f\"checkpoint/MNIST-499-96.24_model.t7\"\n",
    "\n",
    "# Load the checkpoint from the file, mapping the model to the specified device\n",
    "checkpoint = torch.load(model_filename, map_location=device)\n",
    "\n",
    "# Load the model state dictionary from the checkpoint\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(model) # The model has been quantized to int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting weights from layer: conv1_depthwise\n",
      "Weight Shape: (9,), Int8 Weights:\n",
      "[   4   26    4  -49    8   73  -45 -126 -123]\n",
      "\n",
      "Extracting weights from layer: conv1_pointwise\n",
      "Weight Shape: (8,), Int8 Weights:\n",
      "[ 127 -126  127 -128  127 -128 -128  127]\n",
      "\n",
      "Extracting weights from layer: conv2_depthwise\n",
      "Weight Shape: (72,), Int8 Weights:\n",
      "[  -3   36  -24   36   51 -100  127   64  -64   43  -47   23  -21    8\n",
      " -128   21   -1   -9  -77  -42  -20  -18   34 -100  -35 -128  -18  -83\n",
      " -128  -26  -23   41   51    9    5  -62   59   52   78   82  127   95\n",
      "    3  -42  -21   13    7   -3   26    7  -35   20   -8 -128   23   48\n",
      "   49  -92    9   23  -48 -128 -100   25   23  127   47   52  124   17\n",
      "   21   -2]\n",
      "\n",
      "Extracting weights from layer: conv2_pointwise\n",
      "Weight Shape: (80,), Int8 Weights:\n",
      "[  -6   57  -10   18   84  -48   16  127   13  -78   72   21   39    5\n",
      "   -3  127   18    8   22   -2   18   30    0 -127  -67  -21   23   35\n",
      "  -67 -127   17  -86   -1    6  -70  127   55  -38  -22    8   37  -34\n",
      "  102  -57 -128    1   37  -36  -14    4   -7    4 -127  -31    3   -2\n",
      "   10  -20    9  -42 -128   26  -72   52   39  -10   -3   24  -27  127\n",
      "   32  -16   85    8  -95   20 -127  -67   97  -87]\n",
      "\n",
      "Extracting weights from layer: conv3_depthwise\n",
      "Weight Shape: (90,), Int8 Weights:\n",
      "[  21   -9   50  -54   -4   88  -19  113  127   41    2   47  127   10\n",
      "   89   42   10    5   57   -7 -127   -1   -4  -18  -15    5   18  -19\n",
      "  -20   13  -24    0   53   18   37  127   12  -25  -51  127    2    4\n",
      "   68   38   10   69   35  -95   13   -5 -128   55  -11  -34 -127  -43\n",
      "  -58  -30    1   23  -71  -17  -29   21   34    5  -17   -9  -36  -35\n",
      " -128  -51  -20    6 -128  -17   28   27  -23   39   52   57   65  127\n",
      "  -31  -29   -4  -26   22   49]\n",
      "\n",
      "Extracting weights from layer: conv3_pointwise\n",
      "Weight Shape: (120,), Int8 Weights:\n",
      "[ -21  -21  -13  -36    5  -27  -25  127  -13   11   35  -13   45  -46\n",
      "    7 -128  -14   12 -101  -54    2   48   25  -28  -62 -128  -14  -36\n",
      "   39  -18  -49   35  -47  -39   95  127  -13  -18  -63   92  -59 -128\n",
      "   70   -6  -28    2  -16  -28   13    1 -102 -100  104   19  -91  127\n",
      "   32   36 -105   -5  -50  127   18  -42  -61   22  -44   25 -105   -3\n",
      "   -9    6   62    0   -5  -35   15   12  127   -6  -49   59   67    9\n",
      "   40 -127   72   53 -101  -33  -28   58  -59   59  -63  127  -76  -69\n",
      "   11   18    6  -46   30 -107   30   -9  -34  -31  127   82  -43   12\n",
      "  -97  -40  -46  127   80  -22  107   -6]\n",
      "\n",
      "Extracting weights from layer: fc\n",
      "Weight Shape: (120,), Int8 Weights:\n",
      "[ -12    0   54 -128  -27   -8  -64   11   21  -13   15    4  -16  -14\n",
      " -128    1  -11   41  -19  -21   70   39  -28   -1    5   58    3   30\n",
      "   13 -128  -59  -16   32   32  -39    8  -51  -45  -14   17  -22 -128\n",
      "  -21    1   55  -16   35    0   89    4  -29   -3   58   -5    7 -127\n",
      "   -8   23   -8   37 -127   58  -14  -30  -86   69   22   91  -51  -10\n",
      "   48  -99  -22   -1   -3 -128    1   19   -3   19  -65   23    6  -16\n",
      "   49    6   -6   24   53  -76    7  -50  127  -41  -43  -10 -127  -36\n",
      "  -11  -35  -46  -47   34   -9  -21   26   -7   13  127  -43   36   31\n",
      "  -93   25   21  -43  -41  -59  -15   27]\n",
      "\n",
      "The test accuracy after quantization is: 96.24%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset and store the accuracy\n",
    "model.eval()\n",
    "test_accuracy = test_model(model, test_loader)\n",
    "\n",
    "# Extract the int8 weights from all quantized layers in the model and save them to a list\n",
    "quantized_weights_list = print_quantized_weights(model)\n",
    "\n",
    "print(f\"The test accuracy after quantization is: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto4ET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "\n",
    "dataset = 'ECG'\n",
    "\n",
    "# 使用函数创建数据集\n",
    "data_dir = 'raw\\ECG'\n",
    "train_dataset, test_dataset = load_ecg_data(data_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 317 weights that need to be deployed.\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "# Define the number of classes for the classification task\n",
    "num_classes = 2\n",
    "\n",
    "# Initialize the model with the given configuration and number of classes, and move it to the specified device\n",
    "model = NET_4k_ECG(num_classes).to(device)\n",
    "\n",
    "total_weights = count_weights(model)\n",
    "print(f\"The network has {total_weights} weights that need to be deployed.\")\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "model = torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET_4k_ECG(\n",
      "  (quant): Quantize(scale=tensor([0.0372]), zero_point=tensor([46]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (conv1_depthwise): QuantizedConv1d(1, 1, kernel_size=(7,), stride=(1,), scale=0.05332726612687111, zero_point=44, padding=(4,), dilation=(2,))\n",
      "  (conv1_pointwise): QuantizedConv1d(1, 10, kernel_size=(1,), stride=(1,), scale=0.08095549792051315, zero_point=55)\n",
      "  (bn1): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): QuantizedHardswish()\n",
      "  (conv2_depthwise): QuantizedConv1d(10, 10, kernel_size=(5,), stride=(1,), scale=0.12382008880376816, zero_point=66, padding=(1,), groups=10)\n",
      "  (conv2_pointwise): QuantizedConv1d(10, 10, kernel_size=(1,), stride=(1,), scale=0.26861637830734253, zero_point=52)\n",
      "  (activation2): QuantizedHardswish()\n",
      "  (conv3_depthwise): QuantizedConv1d(10, 10, kernel_size=(3,), stride=(1,), scale=0.1977425515651703, zero_point=51, padding=(1,), groups=10)\n",
      "  (conv3_pointwise): QuantizedConv1d(10, 10, kernel_size=(1,), stride=(1,), scale=0.41452309489250183, zero_point=67)\n",
      "  (activation3): QuantizedHardswish()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): QuantizedLinear(in_features=10, out_features=2, scale=0.2551082372665405, zero_point=127, qscheme=torch.per_channel_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "# Specify the filename of the saved model\n",
    "model_filename = f\"checkpoint/ECG-317-99.30_model.t7\"\n",
    "\n",
    "# Load the checkpoint from the file, mapping the model to the specified device\n",
    "checkpoint = torch.load(model_filename, map_location=device)\n",
    "\n",
    "# Load the model state dictionary from the checkpoint\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(model) # The model has been quantized to int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting weights from layer: conv1_depthwise\n",
      "Weight Shape: (7,), Int8 Weights:\n",
      "[-89 127  68  24  -3  -3  -6]\n",
      "\n",
      "Extracting weights from layer: conv1_pointwise\n",
      "Weight Shape: (10,), Int8 Weights:\n",
      "[-127 -128  127  127 -128  127 -128 -128  127 -128]\n",
      "\n",
      "Extracting weights from layer: conv2_depthwise\n",
      "Weight Shape: (50,), Int8 Weights:\n",
      "[ -16  -34 -128  -98  -17   42 -127    1  -36    8  127   26  -47  -33\n",
      " -106  -11  -85  -35  127   -7  -46   29 -128   37  -88  127  -31  -48\n",
      "  -49   30   64  -36  119  127  115   35 -104  -43  127   43 -128  -12\n",
      "    7   11   23   38   22   84  117  127]\n",
      "\n",
      "Extracting weights from layer: conv2_pointwise\n",
      "Weight Shape: (100,), Int8 Weights:\n",
      "[ -11  -34  -70 -128   76  -62  -54    6   24   25  -68   11   37  127\n",
      "  -54  -61   44   85   87   63   -3  105  127  111  109    6   32  114\n",
      "  -97  -36   22  -16  -28   81  -83 -128   15   53  -22  -21    3   57\n",
      "   50  -55   34   13  127  -69  -10   47  -47   33   69  -52  -15    8\n",
      "  127   10  -24    6   -7  -80  117   19   86   74  -62  -80 -127    4\n",
      "   50  -89   66 -125 -125  113   75  -27  -40  127   -6   68   56  -19\n",
      "  111   66  -87  127    6  -92   26   12   92  -78   21  -38   28  127\n",
      "  -29   74]\n",
      "\n",
      "Extracting weights from layer: conv3_depthwise\n",
      "Weight Shape: (30,), Int8 Weights:\n",
      "[   3  -46  127  -22   75 -128   81  -54 -128  -38 -128   97  -32 -128\n",
      "   56    0  127   54  127  -44  -72  -85  127  -17   64  127   63  127\n",
      "   35   32]\n",
      "\n",
      "Extracting weights from layer: conv3_pointwise\n",
      "Weight Shape: (100,), Int8 Weights:\n",
      "[ -12  -51   49   47   29   37 -127   -7  -34  -46   32   -1   50   32\n",
      "   17  127  127   53    5   21   26    1   -7    1  -27  -18   23  -30\n",
      "  127   45  -74  -42  -37  -96   11  -93 -128   40  -56 -107  -23  -24\n",
      "   17  115   29 -127   61  -52    1  -79  -86  -55 -121  127   15  -20\n",
      "  100 -115   57  -29   95  127   -2  -12   63  -70   79  -75  -62  -20\n",
      "   45   82   18  -36  127  -52   33  -49  -29   60   16  110  -28   34\n",
      "  113 -128  -87   25   95  -32   73   30   93   21   17  127  104   34\n",
      "  -51  111]\n",
      "\n",
      "Extracting weights from layer: fc\n",
      "Weight Shape: (20,), Int8 Weights:\n",
      "[  -6  -49   -5   19   10   55  103   17   86 -128 -128  -52   68  107\n",
      "  -90   76  -51 -106  106   28]\n",
      "\n",
      "The test accuracy after quantization is: 99.30%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset and store the accuracy\n",
    "model.eval()\n",
    "test_accuracy = test_model(model, test_loader)\n",
    "\n",
    "# Extract the int8 weights from all quantized layers in the model and save them to a list\n",
    "quantized_weights_list = print_quantized_weights(model)\n",
    "\n",
    "print(f\"The test accuracy after quantization is: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto4ET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

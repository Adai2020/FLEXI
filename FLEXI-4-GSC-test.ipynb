{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'yes' has 2377 audio files.\n",
      "Class 'no' has 2375 audio files.\n",
      "Class 'stop' has 2380 audio files.\n",
      "Class 'on' has 2367 audio files.\n",
      "Class 'off' has 2357 audio files.\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "\n",
    "dataset = 'Google Speech Command (GSC)'\n",
    "\n",
    "data_path = 'raw/speech_commands'\n",
    "labels = ['yes', 'no', 'stop','on','off']\n",
    "\n",
    "train_dataset, test_dataset = load_speech_command_datasets(data_path, labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 465 weights that need to be deployed.\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "# Define the number of classes for the classification task\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize the model with the given configuration and number of classes, and move it to the specified device\n",
    "model = NET_4k_GSC(num_classes).to(device)\n",
    "\n",
    "total_weights = count_weights(model)\n",
    "print(f\"The network has {total_weights} weights that need to be deployed.\")\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "model = torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET_4k_GSC(\n",
      "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (conv1_depthwise): QuantizedConv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), scale=0.11747653037309647, zero_point=98, padding=(1, 1), dilation=(2, 2))\n",
      "  (conv1_pointwise): QuantizedConv2d(1, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.5115418434143066, zero_point=63)\n",
      "  (bn1): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): QuantizedHardswish()\n",
      "  (conv2_depthwise): QuantizedConv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), scale=0.27415165305137634, zero_point=72, padding=(1, 1), dilation=(2, 2), groups=10)\n",
      "  (conv2_pointwise): QuantizedConv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.9521254897117615, zero_point=64)\n",
      "  (bn2): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): QuantizedHardswish()\n",
      "  (conv3_depthwise): QuantizedConv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), scale=0.2711802124977112, zero_point=55, padding=(1, 1), groups=10)\n",
      "  (conv3_pointwise): QuantizedConv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.8606055974960327, zero_point=61)\n",
      "  (bn3): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation3): QuantizedHardswish()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): QuantizedLinear(in_features=10, out_features=5, scale=0.30548372864723206, zero_point=70, qscheme=torch.per_channel_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "# Specify the filename of the saved model\n",
    "model_filename = f\"checkpoint/GSC-465-94.10_model.t7\"\n",
    "\n",
    "# Load the checkpoint from the file, mapping the model to the specified device\n",
    "checkpoint = torch.load(model_filename, map_location=device)\n",
    "\n",
    "# Load the model state dictionary from the checkpoint\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(model) # The model has been quantized to int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting weights from layer: conv1_depthwise\n",
      "Weight Shape: (25,), Int8 Weights:\n",
      "[   6    6   -9  -42   21   23  -17 -128  -74   32   43  -60  -75   -5\n",
      "   23    5  -57  -44  -24    1  -11   56   29    9   14]\n",
      "\n",
      "Extracting weights from layer: conv1_pointwise\n",
      "Weight Shape: (10,), Int8 Weights:\n",
      "[-128 -128  127  118 -128 -128  127 -128  127  127]\n",
      "\n",
      "Extracting weights from layer: conv2_depthwise\n",
      "Weight Shape: (90,), Int8 Weights:\n",
      "[  -7   36  127    1    4  -39  -33  -12   13   53 -128   25   41   -8\n",
      "  -42 -109  111   34   20   54   -3   53  -36  -30   -9   14  127  -15\n",
      "   16   60  103   35   82  -26 -128  -36  -28  -66   61   27  127  -99\n",
      "  -40  -52   30  -28  -43   91  127  -51  -30   50   47  -87   42 -128\n",
      "    1   -2   71  -30    3   45   -6   33  -17    0  -25   71 -128   30\n",
      "   37  -46   78 -124  -79   23  -19  113  -59 -126   48  -55 -128  -64\n",
      "   23  -25   78   46  -36   44]\n",
      "\n",
      "Extracting weights from layer: conv2_pointwise\n",
      "Weight Shape: (100,), Int8 Weights:\n",
      "[ -26 -128   16  -95  -72   71  -84   35  121  127  112  -55  -76 -128\n",
      "  -20  -81   91   37  -75  -98    3  127  -46   20  -94   27  -42  -52\n",
      "    0   96  -54  -73   54   20    3  127  -38   67   43  113  -26   13\n",
      " -128   69  -68  -84  -75   68  -43   62   56   -6  -18  -11   82   -8\n",
      "  -58   21  127   27  105  -74  -79  -49  -94  -58  127  -28   55   73\n",
      "   61   21  -25   10   45   66    3   53  127   84  -29   22  -22 -122\n",
      "   29  -12 -115  127  -27   27   50   48   51  -49   29  -34  -29 -128\n",
      "  -70   36]\n",
      "\n",
      "Extracting weights from layer: conv3_depthwise\n",
      "Weight Shape: (90,), Int8 Weights:\n",
      "[ 117   40  -27   22  111   32    1 -128  -91   78  -42  -24   56 -128\n",
      "  -30  -73 -104  -50  -24  -49  -44  -22  -16  -75  -16   99  127  -23\n",
      "  100   43  -10   31    5  -25 -128  -59  -17   69  115   35    0   19\n",
      " -128  -43  124   -7  -91  -48    4    0   -6   29   94  127  -10  127\n",
      "   89  -16  -11  -49   -2  -66  -36   43   11   81  -22  -59  118   43\n",
      "  -36 -128  -56   73  127   47   -3   13  -64  -47  -99  -25   92   52\n",
      "  -67  -18   66   51  -46 -128]\n",
      "\n",
      "Extracting weights from layer: conv3_pointwise\n",
      "Weight Shape: (100,), Int8 Weights:\n",
      "[ -33  -13   27   93  -25   47  -95   13   25  127  -47   -9   90   18\n",
      "   58   -5    5    4  127    7  -17 -128 -110   79   28  -87  -36   42\n",
      "  -27   66   77   14  -51   43   22   53    2  127   -1  -20   11    5\n",
      "  -48   58   -5  107   28  -35  127    6 -113  -40   72   12  -13 -127\n",
      "  -54   75  -39  -30   29 -128  -73   20  107  -22  -71  -19   40   43\n",
      " -128   96   79    8  -79  -22   51  -24  -19   97  -96  -82   48   13\n",
      "    0   99  127   27  -40   20   57 -108   97   50  127  -28   76   40\n",
      "   20   -4]\n",
      "\n",
      "Extracting weights from layer: fc\n",
      "Weight Shape: (50,), Int8 Weights:\n",
      "[ -78   66  -63  -71  -12  -83 -106  -26   39  127  -76  -67  -20   39\n",
      "   43   -7 -110 -128   89  -15   30   17  -62   -5   28   25  -25  127\n",
      "  -16  -75   11  -27   39   40 -128   52   36  -40  -50  -14   32  -16\n",
      "   52  -58   39  -85   60 -128  -84   81]\n",
      "\n",
      "The test accuracy after quantization is: 94.10%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset and store the accuracy\n",
    "model.eval()\n",
    "test_accuracy = test_model(model, test_loader)\n",
    "\n",
    "# Extract the int8 weights from all quantized layers in the model and save them to a list\n",
    "quantized_weights_list = print_quantized_weights(model)\n",
    "\n",
    "print(f\"The test accuracy after quantization is: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto4ET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

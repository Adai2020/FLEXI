{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading\n",
    "\n",
    "dataset = 'ECG'\n",
    "\n",
    "data_dir = 'raw\\ECG'\n",
    "train_dataset, test_dataset = load_ecg_data(data_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 112 weights that need to be deployed.\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = NET_1k_ECG(num_classes).to(device)\n",
    "\n",
    "total_weights = count_weights(model)\n",
    "print(f\"The network has {total_weights} weights that need to be deployed.\")\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model, inplace=True)\n",
    "model = torch.quantization.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET_1k_ECG(\n",
      "  (quant): Quantize(scale=tensor([0.0371]), zero_point=tensor([46]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (conv1_depthwise): QuantizedConv1d(1, 1, kernel_size=(7,), stride=(1,), scale=0.039918337017297745, zero_point=58, padding=(4,), dilation=(2,))\n",
      "  (conv1_pointwise): QuantizedConv1d(1, 5, kernel_size=(1,), stride=(1,), scale=0.037377890199422836, zero_point=79)\n",
      "  (bn1): QuantizedBatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): QuantizedHardswish()\n",
      "  (conv2_depthwise): QuantizedConv1d(5, 5, kernel_size=(5,), stride=(1,), scale=0.09448608756065369, zero_point=76, padding=(1,), groups=5)\n",
      "  (conv2_pointwise): QuantizedConv1d(5, 5, kernel_size=(1,), stride=(1,), scale=0.2084026336669922, zero_point=64)\n",
      "  (activation2): QuantizedHardswish()\n",
      "  (conv3_depthwise): QuantizedConv1d(5, 5, kernel_size=(3,), stride=(1,), scale=0.1903160959482193, zero_point=57, padding=(1,), groups=5)\n",
      "  (conv3_pointwise): QuantizedConv1d(5, 5, kernel_size=(1,), stride=(1,), scale=0.3313724994659424, zero_point=27)\n",
      "  (activation3): QuantizedHardswish()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): QuantizedLinear(in_features=5, out_features=2, scale=0.19317609071731567, zero_point=98, qscheme=torch.per_channel_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "\n",
    "# Specify the filename of the saved model\n",
    "model_filename = f\"checkpoint/ECG-112-99.20_model.t7\"\n",
    "\n",
    "# Load the checkpoint from the file, mapping the model to the specified device\n",
    "checkpoint = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "print(model) # The model has been quantized to int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting weights from layer: conv1_depthwise\n",
      "Weight Shape: (7,), Int8 Weights:\n",
      "[-23 -41 127  -7  23  -5  20]\n",
      "\n",
      "Extracting weights from layer: conv1_pointwise\n",
      "Weight Shape: (5,), Int8 Weights:\n",
      "[ 127 -128 -128 -128 -127]\n",
      "\n",
      "Extracting weights from layer: conv2_depthwise\n",
      "Weight Shape: (25,), Int8 Weights:\n",
      "[ -83 -128 -102  110   70 -128   14   39   42  -16   31   28 -128  -39\n",
      "  -44 -128    3   50  -54   76   29  127 -122  -34   18]\n",
      "\n",
      "Extracting weights from layer: conv2_pointwise\n",
      "Weight Shape: (25,), Int8 Weights:\n",
      "[-128   21  -73  -92  -15   36   74   13  127  -62   51  -12  -68  -32\n",
      " -127   24  127  -48   37  -89 -113  -33  -15 -128   51]\n",
      "\n",
      "Extracting weights from layer: conv3_depthwise\n",
      "Weight Shape: (15,), Int8 Weights:\n",
      "[-119 -127   48  -16  127   50 -102  127  -66  127  110   89   53  127\n",
      "   62]\n",
      "\n",
      "Extracting weights from layer: conv3_pointwise\n",
      "Weight Shape: (25,), Int8 Weights:\n",
      "[ -22  -83 -128   99  -23   59  -50    7  -24 -128   -7  -50   29  127\n",
      "  -92  -85 -101   37 -128   63   -5   56  127   68  -44]\n",
      "\n",
      "Extracting weights from layer: fc\n",
      "Weight Shape: (10,), Int8 Weights:\n",
      "[-127   89  -35  126  -22  -26   38   23  -95  127]\n",
      "\n",
      "The test accuracy after quantization is: 99.20%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset and store the accuracy\n",
    "model.eval()\n",
    "test_accuracy = test_model(model, test_loader)\n",
    "\n",
    "# Extract the int8 weights from all quantized layers in the model and save them to a list\n",
    "quantized_weights_list = print_quantized_weights(model)\n",
    "\n",
    "print(f\"The test accuracy after quantization is: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto4ET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
